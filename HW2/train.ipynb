{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJxJHruNLb7Y"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gkGnB9WyHSXB",
    "outputId": "65a2f2e8-12c1-490e-df08-f9087cf8f7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mmcv-full in /opt/conda/lib/python3.7/site-packages (1.3.17)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (21.0)\n",
      "Requirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (4.5.4.58)\n",
      "Requirement already satisfied: addict in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (2.4.0)\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (0.31.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (1.21.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (6.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from mmcv-full) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->mmcv-full) (3.0.3)\n",
      "Cloning into 'mmdetection'...\n",
      "remote: Enumerating objects: 21799, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 21799 (delta 6), reused 9 (delta 3), pack-reused 21781\u001b[K\n",
      "Receiving objects: 100% (21799/21799), 25.21 MiB | 10.35 MiB/s, done.\n",
      "Resolving deltas: 100% (15311/15311), done.\n",
      "/home/ueidarchen/Homework 2/mmdetection\n",
      "Obtaining file:///home/ueidarchen/Homework%202/mmdetection\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1) (3.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1) (1.21.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1) (1.16.0)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1) (3.1.0)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1) (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1) (3.0.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools->mmdet==2.18.1) (58.3.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools->mmdet==2.18.1) (0.29.24)\n",
      "Installing collected packages: mmdet\n",
      "  Running setup.py develop for mmdet\n",
      "Successfully installed mmdet-2.18.1\n"
     ]
    }
   ],
   "source": [
    "# Install mmcv\n",
    "!pip install mmcv-full\n",
    "\n",
    "# Install mmdetection\n",
    "!rm -rf mmdetection\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SupSgXQzswt",
    "outputId": "2e279fdf-2b86-443b-8e1e-1529e21103c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/1/uc?id=1lrKueI4HrySQDGvpkilQN9BfaMUN7hZi\n",
      "To: /home/ueidarchen/Homework 2/mmdetection/train.zip\n",
      "100%|█████████████████████████████████████████| 420M/420M [00:01<00:00, 266MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the training data\n",
    "!gdown https://drive.google.com/u/1/uc?id=1lrKueI4HrySQDGvpkilQN9BfaMUN7hZi\n",
    "!unzip -q train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VRDL'...\n",
      "remote: Enumerating objects: 32, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 32 (delta 5), reused 28 (delta 4), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (32/32), done.\n",
      "Requirement already satisfied: h5py==3.6.0 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: mmcv==1.3.18 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 2)) (1.3.18)\n",
      "Requirement already satisfied: mmcv_full==1.3.17 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 3)) (1.3.17)\n",
      "Requirement already satisfied: mmdet==2.18.1 in /home/ueidarchen/Homework 2/mmdetection (from -r VRDL/HW2/requirements.txt (line 4)) (2.18.1)\n",
      "Requirement already satisfied: numpy==1.21.3 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 5)) (1.21.3)\n",
      "Requirement already satisfied: Pillow==8.4.0 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 6)) (8.4.0)\n",
      "Requirement already satisfied: torch==1.10.0 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 7)) (1.10.0)\n",
      "Requirement already satisfied: torchvision==0.11.1 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 8)) (0.11.1)\n",
      "Requirement already satisfied: googledrivedownloader==0.4 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: opencv_python==4.5.4.58 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 10)) (4.5.4.58)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /opt/conda/lib/python3.7/site-packages (from -r VRDL/HW2/requirements.txt (line 11)) (4.62.3)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py==3.6.0->-r VRDL/HW2/requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: addict in /opt/conda/lib/python3.7/site-packages (from mmcv==1.3.18->-r VRDL/HW2/requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.7/site-packages (from mmcv==1.3.18->-r VRDL/HW2/requirements.txt (line 2)) (0.31.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mmcv==1.3.18->-r VRDL/HW2/requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmcv==1.3.18->-r VRDL/HW2/requirements.txt (line 2)) (21.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (from mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.10.0->-r VRDL/HW2/requirements.txt (line 7)) (3.10.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (58.3.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools->mmdet==2.18.1->-r VRDL/HW2/requirements.txt (line 4)) (0.29.24)\n"
     ]
    }
   ],
   "source": [
    "# Clone project\n",
    "!rm -rf VRDL\n",
    "!git clone https://github.com/udchen19/VRDL.git\n",
    "!pip install -r VRDL/HW2/requirements.txt\n",
    "!cp VRDL/HW2/config.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hD0mmMixT0p",
    "outputId": "94a1aa81-5824-488e-de29-a6919fa75929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102 True\n",
      "2.18.1\n",
      "10.2\n",
      "GCC 7.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi9zw03oM4CH"
   },
   "source": [
    "## Perform inference with a MMDet detector\n",
    "MMDetection already provides high level APIs to do inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kP-3TdmUyv9R"
   },
   "outputs": [],
   "source": [
    "# Mat file preprocessing\n",
    "import h5py, json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "DATA_DIR = 'train'\n",
    "\n",
    "mat = h5py.File(f'{DATA_DIR}/digitStruct.mat', 'r')\n",
    "bboxes = mat['digitStruct/bbox']\n",
    "names = mat['digitStruct/name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eH9eersJyv9R"
   },
   "outputs": [],
   "source": [
    "# Get dict list from mat file\n",
    "\n",
    "KEYS = ['label', 'left', 'top', 'width', 'height']\n",
    "ann_list = []\n",
    "\n",
    "for bbox, name in zip(bboxes, names):\n",
    "    filename = ''.join(chr(char[0]) for char in mat[name[0]][()])\n",
    "    img = Image.open(f'{DATA_DIR}/{filename}')\n",
    "    width, height = img.size\n",
    "    if mat[bbox[0]][KEYS[0]].shape[0] > 1:\n",
    "        attr = np.array([[mat[mat[bbox[0]][key][k][0]][0][0]\n",
    "                          for k in range(mat[bbox[0]][key].shape[0])]\n",
    "                             for key in KEYS]).T.astype(int)\n",
    "    else:\n",
    "        attr = np.array([mat[bbox[0]][key][0] for key in KEYS]).T.astype(int)\n",
    "    labels = (attr[:, 0] % 10).tolist()\n",
    "    bboxes_data = attr[:, 1:].tolist()\n",
    "    obj = dict(\n",
    "        filename = filename,\n",
    "        width = width,\n",
    "        height = height,\n",
    "        ann = dict(\n",
    "            bboxes = bboxes_data,\n",
    "            labels = labels\n",
    "        )\n",
    "    )\n",
    "    ann_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qsCahVVjyv9S"
   },
   "outputs": [],
   "source": [
    "# Export dict list to JSON\n",
    "\n",
    "import random, json\n",
    "\n",
    "DATA_SIZE = len(ann_list)\n",
    "TRAIN_RATIO = 0.7\n",
    "TRAIN_SIZE = int(DATA_SIZE * TRAIN_RATIO)\n",
    "VAL_SIZE = DATA_SIZE - TRAIN_SIZE\n",
    "\n",
    "img_all = [ann['filename'] for ann in ann_list]\n",
    "random.shuffle(img_all)\n",
    "img_train = set(img_all[: TRAIN_SIZE])\n",
    "img_val = set(img_all[TRAIN_SIZE: ])\n",
    "ann_train = [ann for ann in ann_list if ann['filename'] in img_train]\n",
    "ann_val = [ann for ann in ann_list if ann['filename'] in img_val]\n",
    "\n",
    "with open(f'{DATA_DIR}/ann_train.json', 'w') as f:\n",
    "    f.write(json.dumps(ann_train))\n",
    "with open(f'{DATA_DIR}/ann_val.json', 'w') as f:\n",
    "    f.write(json.dumps(ann_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x23leyHEyv9S"
   },
   "outputs": [],
   "source": [
    "# Define and register custom dataset\n",
    "\n",
    "import os.path as osp\n",
    "import copy\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.custom import CustomDataset\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class DigitDataset(CustomDataset):\n",
    "    CLASSES = tuple(str(i) for i in range(10))\n",
    "    def load_annotations(self, ann_file):\n",
    "        ann_list = mmcv.load(self.ann_file)\n",
    "        for i, a in enumerate(ann_list):\n",
    "            ann_list[i]['ann']['bboxes'] = np.array(\n",
    "                ann_list[i]['ann']['bboxes'], dtype=np.float32)\n",
    "            # (x, y, w, h) -> #(xmin, ymin, xmax, ymax)\n",
    "            ann_list[i]['ann']['bboxes'][:, 2:] += ann_list[i]['ann']['bboxes'][:, :2] - 1\n",
    "            ann_list[i]['ann']['labels'] = np.array(\n",
    "                ann_list[i]['ann']['labels'], dtype=np.long)\n",
    "        return ann_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxU8Q2MvFkgL",
    "outputId": "62f7e465-76e6-4b42-ac6b-584efb773f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-25 16:31:18--  https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.5\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 167287506 (160M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/faster_rcnn.pth’\n",
      "\n",
      "checkpoints/faster_ 100%[===================>] 159.54M  68.3MB/s    in 2.3s    \n",
      "\n",
      "2021-11-25 16:31:20 (68.3 MB/s) - ‘checkpoints/faster_rcnn.pth’ saved [167287506/167287506]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Pretrained weights\n",
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth -O checkpoints/faster_rcnn.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hamZrlnH-YDD",
    "outputId": "53a92882-2dea-4faf-d985-ef7f14daecb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    type='FasterRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=10,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=False,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'DigitDataset'\n",
      "data_root = 'train/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(512, 512),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='DigitDataset',\n",
      "        ann_file='train/ann_train.json',\n",
      "        img_prefix='train/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='DigitDataset',\n",
      "        ann_file='train/ann_val.json',\n",
      "        img_prefix='train/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(512, 512),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='DigitDataset',\n",
      "        ann_file='train/ann_val.json',\n",
      "        img_prefix='train/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(512, 512),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='mAP')\n",
      "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='CosineAnnealing',\n",
      "    warmup='linear',\n",
      "    warmup_iters=1000,\n",
      "    warmup_ratio=0.1,\n",
      "    min_lr_ratio=1e-05)\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=20)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(\n",
      "    interval=50,\n",
      "    hooks=[dict(type='TextLoggerHook'),\n",
      "           dict(type='TensorboardLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "gpu_ids = range(0, 1)\n",
      "seed = 19\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'DEBUG'\n",
      "load_from = 'checkpoints/faster_rcnn.pth'\n",
      "resume_from = None\n",
      "work_dir = './tutorial_exps'\n",
      "workflow = [('train', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Configuration File\n",
    "from mmcv import Config\n",
    "cfg = Config.fromfile('config.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WBWHu010PN3",
    "outputId": "7bcee05b-bbee-48cb-9979-cd78cfeea083"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/ueidarchen/Homework 2/mmdetection/mmdet/datasets/custom.py:157: UserWarning: CustomDataset does not support filtering empty gt images.\n",
      "  'CustomDataset does not support filtering empty gt images.')\n",
      "2021-11-25 16:31:25,359 - mmdet - INFO - load checkpoint from local path: checkpoints/faster_rcnn.pth\n",
      "2021-11-25 16:31:25,483 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([11, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([11]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([40, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([40]).\n",
      "2021-11-25 16:31:25,486 - mmdet - INFO - Start running, host: ueidarchen@udchen-vrdl1, work_dir: /home/ueidarchen/Homework 2/mmdetection/tutorial_exps\n",
      "2021-11-25 16:31:25,487 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       \n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "2021-11-25 16:31:25,488 - mmdet - INFO - workflow: [('train', 1)], max: 20 epochs\n",
      "2021-11-25 16:31:25,488 - mmdet - INFO - Checkpoints will be saved to /home/ueidarchen/Homework 2/mmdetection/tutorial_exps by HardDiskBackend.\n"
     ]
    }
   ],
   "source": [
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "\n",
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the detector\n",
    "model = build_detector(\n",
    "    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "# Train detector\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_vYQF5K2NqqI",
    "MfQ-yspZLuuI"
   ],
   "name": "0816035_HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
